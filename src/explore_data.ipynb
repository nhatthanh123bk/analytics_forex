{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Forex predict using linear regression')\\\n",
    "        .config('hive.metastore.uris', 'thrift://10.121.31.83:9083')\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|      person|\n",
      "|       test2|\n",
      "|     titanic|\n",
      "|  titanic_db|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"Database 'forex' not found;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fbe4cf412263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use forex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from 2012_m1_eurusd limit 10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select count(distinct time) as count_time from 2012_m1_eurusd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select count(distinct date) as count_time from 2012_m1_eurusd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select count(distinct volume) as count_time from 2012_m1_eurusd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \"\"\"\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"Database 'forex' not found;\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"use forex\")\n",
    "spark.sql(\"select * from 2012_m1_eurusd limit 10\").show()\n",
    "spark.sql(\"select count(distinct time) as count_time from 2012_m1_eurusd\").show()\n",
    "spark.sql(\"select count(distinct date) as count_time from 2012_m1_eurusd\").show()\n",
    "spark.sql(\"select count(distinct volume) as count_time from 2012_m1_eurusd\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dữ liệu bắt đầu từ ngày 02-1-2012 đến ngày 31-12-2012\n",
    "- Trong 1 ngày dữ liệu sẽ được cập nhật theo từng phút, bắt đầu từ 00.00 đến 23:59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max(time)|\n",
      "+---------+\n",
      "|    23:59|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|min(time)|\n",
      "+---------+\n",
      "|    00:00|\n",
      "+---------+\n",
      "\n",
      "+----------+\n",
      "| max(date)|\n",
      "+----------+\n",
      "|2012.12.31|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "| min(date)|\n",
      "+----------+\n",
      "|2012.01.02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT MAX(time) FROM 2012_m1_eurusd\").show()\n",
    "spark.sql(\"SELECT MIN(time) FROM 2012_m1_eurusd\").show()\n",
    "spark.sql(\"SELECT MAX(date) FROM 2012_m1_eurusd\").show()\n",
    "spark.sql(\"SELECT MIN(date) FROM 2012_m1_eurusd\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|min(high_bid)|\n",
      "+-------------+\n",
      "|      1.20474|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|min(open_bid)|\n",
      "+-------------+\n",
      "|      1.20448|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|max(high_bid)|\n",
      "+-------------+\n",
      "|      1.34856|\n",
      "+-------------+\n",
      "\n",
      "+----------+-----+------------+\n",
      "|      date| time|min_high_bid|\n",
      "+----------+-----+------------+\n",
      "|2012.07.24|12:32|     1.20474|\n",
      "+----------+-----+------------+\n",
      "\n",
      "+----------+-----+--------+------------+-------+---------+\n",
      "|      date| time|open_bid|max_high_bid|low_bid|close_bid|\n",
      "+----------+-----+--------+------------+-------+---------+\n",
      "|2012.02.24|13:04| 1.34782|     1.34856|1.34782|  1.34806|\n",
      "+----------+-----+--------+------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT MIN(high_bid) FROM 2012_m1_eurusd\").show()\n",
    "spark.sql(\"SELECT MIN(open_bid) FROM 2012_m1_eurusd\").show()\n",
    "spark.sql(\"SELECT MAX(high_bid) FROM 2012_m1_eurusd\").show()\n",
    "spark.sql(\"SELECT date,time,high_bid as min_high_bid FROM 2012_m1_eurusd WHERE high_bid=1.20474\").show()\n",
    "spark.sql(\"SELECT date,time,open_bid,high_bid as max_high_bid,low_bid,close_bid FROM 2012_m1_eurusd WHERE high_bid=1.34856\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xóa column close_bid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+--------+-------+---------+\n",
      "|      date| time|open_bid|high_bid|low_bid|close_bid|\n",
      "+----------+-----+--------+--------+-------+---------+\n",
      "|2012.01.02|02:00| 1.29324| 1.29381|1.29324|  1.29332|\n",
      "|2012.01.02|02:01| 1.29326| 1.29345|1.29275|  1.29341|\n",
      "|2012.01.02|02:02| 1.29342| 1.29344|1.29341|  1.29343|\n",
      "|2012.01.02|02:03| 1.29342| 1.29342|1.29335|  1.29336|\n",
      "|2012.01.02|02:04| 1.29324| 1.29347| 1.2932|  1.29346|\n",
      "|2012.01.02|02:05| 1.29354| 1.29415|1.29354|  1.29392|\n",
      "|2012.01.02|02:06| 1.29393| 1.29442|1.29391|   1.2943|\n",
      "|2012.01.02|02:07| 1.29429| 1.29429|1.29395|  1.29395|\n",
      "|2012.01.02|02:08| 1.29396| 1.29408|1.29395|  1.29407|\n",
      "|2012.01.02|02:09| 1.29406| 1.29434|1.29401|   1.2942|\n",
      "|2012.01.02|02:10| 1.29418| 1.29436|1.29408|  1.29425|\n",
      "|2012.01.02|02:11| 1.29423| 1.29438|1.29423|  1.29433|\n",
      "|2012.01.02|02:12| 1.29442| 1.29447|1.29401|  1.29432|\n",
      "|2012.01.02|02:13| 1.29433| 1.29437|1.29421|  1.29422|\n",
      "|2012.01.02|02:14| 1.29421| 1.29435|1.29403|  1.29405|\n",
      "|2012.01.02|02:15| 1.29404| 1.29416|1.29404|  1.29416|\n",
      "|2012.01.02|02:16|  1.2942| 1.29427| 1.2942|  1.29423|\n",
      "|2012.01.02|02:17| 1.29426| 1.29426|1.29422|  1.29425|\n",
      "|2012.01.02|02:18| 1.29427| 1.29427|1.29417|   1.2942|\n",
      "|2012.01.02|02:19| 1.29419| 1.29419|1.29419|  1.29419|\n",
      "+----------+-----+--------+--------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_forex = spark.sql(\"SELECT * FROM 2012_m1_eurusd\")\n",
    "df_forex = df_forex.drop('volume')\n",
    "df_forex.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
